{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] 系统找不到指定的文件。: '9444'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m9444\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 系统找不到指定的文件。: '9444'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('9444')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQAv2 dataset preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('D:\\LSL\\9444\\VQA')\n",
    "from PythonHelperTools.vqaTools import vqa # VQA cloned from VQAv2's github with someone's modification to work in python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/d/LSL/9444\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "with open(os.path.join('dataset', 'merged_data.json'), 'r') as f:\n",
    "    data1 =json.load(f)\n",
    "\n",
    "with open(os.path.join('VQA', 'Annotations', 'v2_mscoco_val2014_annotations.json'), 'r') as f:\n",
    "    data2 =json.load(f)\n",
    "\n",
    "with open(os.path.join('VQA', 'Questions', 'v2_OpenEnded_mscoco_val2014_questions.json'), 'r') as f:\n",
    "    data3 =json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-level keys: dict_keys(['info', 'license', 'data_subtype', 'annotations', 'data_type'])\n",
      "{'answer_type': 'other',\n",
      " 'answers': [{'answer': 'down', 'answer_confidence': 'yes', 'answer_id': 1},\n",
      "             {'answer': 'down', 'answer_confidence': 'yes', 'answer_id': 2},\n",
      "             {'answer': 'at table', 'answer_confidence': 'yes', 'answer_id': 3},\n",
      "             {'answer': 'skateboard',\n",
      "              'answer_confidence': 'yes',\n",
      "              'answer_id': 4},\n",
      "             {'answer': 'down', 'answer_confidence': 'yes', 'answer_id': 5},\n",
      "             {'answer': 'table', 'answer_confidence': 'yes', 'answer_id': 6},\n",
      "             {'answer': 'down', 'answer_confidence': 'yes', 'answer_id': 7},\n",
      "             {'answer': 'down', 'answer_confidence': 'yes', 'answer_id': 8},\n",
      "             {'answer': 'down', 'answer_confidence': 'yes', 'answer_id': 9},\n",
      "             {'answer': 'down', 'answer_confidence': 'yes', 'answer_id': 10}],\n",
      " 'image_id': 262148,\n",
      " 'multiple_choice_answer': 'down',\n",
      " 'question_id': 262148000,\n",
      " 'question_type': 'none of the above'}\n"
     ]
    }
   ],
   "source": [
    "# Print the top-level keys\n",
    "print(\"Top-level keys:\", data2.keys() if isinstance(data2, dict) else \"Data is a list\")\n",
    "\n",
    "pprint(data2['annotations'][0])\n",
    "pprint(data2['annotations'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_type': 'other',\n",
      " 'answers': [{'answer': 'table', 'answer_confidence': 'yes', 'answer_id': 1},\n",
      "             {'answer': 'table', 'answer_confidence': 'yes', 'answer_id': 2},\n",
      "             {'answer': 'table', 'answer_confidence': 'yes', 'answer_id': 3},\n",
      "             {'answer': 'picnic table',\n",
      "              'answer_confidence': 'yes',\n",
      "              'answer_id': 4},\n",
      "             {'answer': 'picnic table',\n",
      "              'answer_confidence': 'yes',\n",
      "              'answer_id': 5},\n",
      "             {'answer': 'picnic table',\n",
      "              'answer_confidence': 'yes',\n",
      "              'answer_id': 6},\n",
      "             {'answer': 'picnic table',\n",
      "              'answer_confidence': 'yes',\n",
      "              'answer_id': 7},\n",
      "             {'answer': 'picnic table',\n",
      "              'answer_confidence': 'yes',\n",
      "              'answer_id': 8},\n",
      "             {'answer': 'skateboard',\n",
      "              'answer_confidence': 'yes',\n",
      "              'answer_id': 9},\n",
      "             {'answer': 'picnic table',\n",
      "              'answer_confidence': 'yes',\n",
      "              'answer_id': 10}],\n",
      " 'image_id': 262148,\n",
      " 'multiple_choice_answer': 'picnic table',\n",
      " 'question_id': 262148002,\n",
      " 'question_type': 'what is'}\n"
     ]
    }
   ],
   "source": [
    "pprint(data2['annotations'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answerable': 1,\n",
       " 'image': 262148,\n",
       " 'question': 'Where is he looking?',\n",
       " 'answer_type': 'other',\n",
       " 'answers': [{'answer': 'down', 'answer_confidence': 'yes'},\n",
       "  {'answer': 'down', 'answer_confidence': 'yes'},\n",
       "  {'answer': 'at table', 'answer_confidence': 'yes'},\n",
       "  {'answer': 'skateboard', 'answer_confidence': 'yes'},\n",
       "  {'answer': 'down', 'answer_confidence': 'yes'},\n",
       "  {'answer': 'table', 'answer_confidence': 'yes'},\n",
       "  {'answer': 'down', 'answer_confidence': 'yes'},\n",
       "  {'answer': 'down', 'answer_confidence': 'yes'},\n",
       "  {'answer': 'down', 'answer_confidence': 'yes'},\n",
       "  {'answer': 'down', 'answer_confidence': 'yes'}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-level keys: dict_keys(['info', 'task_type', 'data_type', 'license', 'data_subtype', 'questions'])\n",
      "{'image_id': 262148,\n",
      " 'question': 'Where is he looking?',\n",
      " 'question_id': 262148000}\n"
     ]
    }
   ],
   "source": [
    "print(\"Top-level keys:\", data3.keys() if isinstance(data3, dict) else \"Data is a list\")\n",
    "\n",
    "# for key in data3.keys():\n",
    "#     pprint(data3[key])\n",
    "pprint(data3['questions'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def merge_question_answer(question_file, answer_file):\n",
    "    with open(question_file, 'r') as f:\n",
    "        question_data = json.load(f)\n",
    "\n",
    "    with open(answer_file, 'r') as f:\n",
    "        answer_data = json.load(f)\n",
    "\n",
    "    merged_data = []\n",
    "\n",
    "    answer_dict = {annotation['question_id']: annotation for annotation in answer_data['annotations']}\n",
    "\n",
    "    for question in question_data['questions']:\n",
    "        qid = question['question_id']\n",
    "\n",
    "        if qid in answer_dict:\n",
    "            answer_entry = answer_dict[qid]['multiple_choice_answer']\n",
    "\n",
    "            merged_entry = {\n",
    "                \"image_id\": question['image_id'],\n",
    "                \"question\": question['question'],\n",
    "                \"answer\": answer_entry\n",
    "            }\n",
    "            merged_data.append(merged_entry)\n",
    "\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image_id': 262148, 'question': 'Where is he looking?', 'answer': 'down'}, {'image_id': 262148, 'question': 'What are the people in the background doing?', 'answer': 'watching'}]\n"
     ]
    }
   ],
   "source": [
    "question_file = os.path.join('VQA', 'Questions', 'v2_OpenEnded_mscoco_val2014_questions.json')\n",
    "answer_file = os.path.join('VQA', 'Annotations', 'v2_mscoco_val2014_annotations.json')\n",
    "\n",
    "merged_data = merge_question_answer(question_file, answer_file)\n",
    "with open(os.path.join('dataset', 'merged_data_2.json'), 'w') as f:\n",
    "    json.dump(merged_data, f, indent=4)\n",
    "\n",
    "print(merged_data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom class VQADataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    \"\"\"VQA (v2) dataset.\"\"\"\n",
    "    def __init__(self, dataset_load, processor_load):\n",
    "        self.dataset = dataset_load\n",
    "        self.processor = processor_load\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.dataset[idx]['question']\n",
    "        answer = self.dataset[idx]['answer']\n",
    "        image_id = self.dataset[idx]['image_id']\n",
    "        image_files_dir = os.path.join('dataset', 'val2014')\n",
    "        image_name = 'COCO_val2014_' + '0' * (12 - len(str(image_id))) + str(image_id) + '.jpg'\n",
    "        image_path = os.path.join(image_files_dir, image_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        text = question\n",
    "        \n",
    "        encoding = self.processor(image, text, padding=\"max_length\", truncation=True, return_tensors=\"pt\", return_attention_mask=True)\n",
    "        labels = self.processor.tokenizer.encode(\n",
    "            answer,\n",
    "            max_length= 16,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        encoding[\"labels\"] = labels\n",
    "        # remove batch dimension\n",
    "        for k,v in encoding.items():  encoding[k] = v.squeeze()\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'configs/med_config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m IMAGE_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m320\u001b[39m\n\u001b[0;32m      8\u001b[0m check_point_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_base_capfilt_large.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mblip_vqa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_point_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbase\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m processor \u001b[38;5;241m=\u001b[39m blip_feature_extractor(pretrained\u001b[38;5;241m=\u001b[39mcheck_point_path, image_size\u001b[38;5;241m=\u001b[39mIMAGE_SIZE, vit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\LSL\\BLIP\\models\\blip_vqa.py:171\u001b[0m, in \u001b[0;36mblip_vqa\u001b[1;34m(pretrained, **kwargs)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mblip_vqa\u001b[39m(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 171\u001b[0m     model \u001b[38;5;241m=\u001b[39m BLIP_VQA(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n\u001b[0;32m    173\u001b[0m         model,msg \u001b[38;5;241m=\u001b[39m load_checkpoint(model,pretrained)\n",
      "File \u001b[1;32mD:\\LSL\\BLIP\\models\\blip_vqa.py:29\u001b[0m, in \u001b[0;36mBLIP_VQA.__init__\u001b[1;34m(self, med_config, image_size, vit, vit_grad_ckpt, vit_ckpt_layer)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual_encoder, vision_width \u001b[38;5;241m=\u001b[39m create_vit(vit, image_size, vit_grad_ckpt, vit_ckpt_layer, drop_path_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m init_tokenizer()  \n\u001b[1;32m---> 29\u001b[0m encoder_config \u001b[38;5;241m=\u001b[39m \u001b[43mBertConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmed_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m encoder_config\u001b[38;5;241m.\u001b[39mencoder_width \u001b[38;5;241m=\u001b[39m vision_width\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_encoder \u001b[38;5;241m=\u001b[39m BertModel(config\u001b[38;5;241m=\u001b[39mencoder_config, add_pooling_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \n",
      "File \u001b[1;32mD:\\LSL\\anaconda3\\envs\\BLIP_env\\lib\\site-packages\\transformers\\configuration_utils.py:765\u001b[0m, in \u001b[0;36mPretrainedConfig.from_json_file\u001b[1;34m(cls, json_file)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_json_file\u001b[39m(\u001b[38;5;28mcls\u001b[39m, json_file: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPretrainedConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    754\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;124;03m    Instantiates a [`PretrainedConfig`] from the path to a JSON file of parameters.\u001b[39;00m\n\u001b[0;32m    756\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    763\u001b[0m \n\u001b[0;32m    764\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 765\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dict_from_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n",
      "File \u001b[1;32mD:\\LSL\\anaconda3\\envs\\BLIP_env\\lib\\site-packages\\transformers\\configuration_utils.py:770\u001b[0m, in \u001b[0;36mPretrainedConfig._dict_from_json_file\u001b[1;34m(cls, json_file)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dict_from_json_file\u001b[39m(\u001b[38;5;28mcls\u001b[39m, json_file: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike]):\n\u001b[1;32m--> 770\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[0;32m    771\u001b[0m         text \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(text)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'configs/med_config.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"D:\\LSL\") # add path to python env to find BLIP folder\n",
    "# from BLIP.models import blip_vqa\n",
    "from BLIP.models.blip_vqa import blip_vqa\n",
    "\n",
    "IMAGE_SIZE = 320\n",
    "check_point_path = os.path.join('.', 'model_base_capfilt_large.pth')\n",
    "model = blip_vqa(pretrained=check_point_path, image_size=IMAGE_SIZE, vit='base')\n",
    "model = model.to(device)\n",
    "\n",
    "processor = blip_feature_extractor(pretrained=check_point_path, image_size=IMAGE_SIZE, vit='base')\n",
    "processor = processor.to(device)\n",
    "c\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a63e8eee50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "\n",
    "# model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-capfilt-large\", cache_dir='BLIP_checkpoints\\\\')\n",
    "# processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-capfilt-large\", cache_dir='BLIP_checkpoints\\\\')\n",
    "# model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-capfilt-large\", cache_dir='BLIP_checkpoints\\\\')\n",
    "# processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-capfilt-large\", cache_dir='BLIP_checkpoints\\\\')\n",
    "\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\", cache_dir='BLIP_checkpoints\\\\')\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\", cache_dir='BLIP_checkpoints\\\\')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset \n",
    "\n",
    "dataset_json_path = os.path.join('dataset', 'merged_data_2.json')\n",
    "training_data = load_dataset(\"json\", data_files=dataset_json_path, split=\"train[:70%]\")\n",
    "valid_data = load_dataset(\"json\", data_files=dataset_json_path, split=\"train[70%:]\")\n",
    "\n",
    "training_set = VQADataset(dataset_load=training_data, processor_load=processor)\n",
    "valid_set = VQADataset(dataset_load=valid_data, processor_load=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 12\n",
    "train_dataloader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def custom_collate(batch):\n",
    "    # Separate each element in the batch\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)  # Adjust padding_value as needed\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)  # Use -100 for ignored labels in NLP\n",
    "\n",
    "    # Stack other fixed-size tensors\n",
    "    pixel_values_stacked = torch.stack(pixel_values)\n",
    "    attention_mask_padded = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Return the batch as a dictionary\n",
    "    return {\n",
    "        \"input_ids\": input_ids_padded,\n",
    "        \"pixel_values\": pixel_values_stacked,\n",
    "        \"attention_mask\": attention_mask_padded,\n",
    "        \"labels\": labels_padded,\n",
    "    }\n",
    "\n",
    "# Use this collate function in your DataLoader\n",
    "train_dataloader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vision_model\n",
      "vision_model.embeddings\n",
      "vision_model.embeddings.patch_embedding\n",
      "vision_model.encoder\n",
      "vision_model.encoder.layers\n",
      "vision_model.encoder.layers.0\n",
      "vision_model.encoder.layers.0.self_attn\n",
      "vision_model.encoder.layers.0.self_attn.dropout\n",
      "vision_model.encoder.layers.0.self_attn.qkv\n",
      "vision_model.encoder.layers.0.self_attn.projection\n",
      "vision_model.encoder.layers.0.layer_norm1\n",
      "vision_model.encoder.layers.0.mlp\n",
      "vision_model.encoder.layers.0.mlp.activation_fn\n",
      "vision_model.encoder.layers.0.mlp.fc1\n",
      "vision_model.encoder.layers.0.mlp.fc2\n",
      "vision_model.encoder.layers.0.layer_norm2\n",
      "vision_model.encoder.layers.1\n",
      "vision_model.encoder.layers.1.self_attn\n",
      "vision_model.encoder.layers.1.self_attn.dropout\n",
      "vision_model.encoder.layers.1.self_attn.qkv\n",
      "vision_model.encoder.layers.1.self_attn.projection\n",
      "vision_model.encoder.layers.1.layer_norm1\n",
      "vision_model.encoder.layers.1.mlp\n",
      "vision_model.encoder.layers.1.mlp.activation_fn\n",
      "vision_model.encoder.layers.1.mlp.fc1\n",
      "vision_model.encoder.layers.1.mlp.fc2\n",
      "vision_model.encoder.layers.1.layer_norm2\n",
      "vision_model.encoder.layers.2\n",
      "vision_model.encoder.layers.2.self_attn\n",
      "vision_model.encoder.layers.2.self_attn.dropout\n",
      "vision_model.encoder.layers.2.self_attn.qkv\n",
      "vision_model.encoder.layers.2.self_attn.projection\n",
      "vision_model.encoder.layers.2.layer_norm1\n",
      "vision_model.encoder.layers.2.mlp\n",
      "vision_model.encoder.layers.2.mlp.activation_fn\n",
      "vision_model.encoder.layers.2.mlp.fc1\n",
      "vision_model.encoder.layers.2.mlp.fc2\n",
      "vision_model.encoder.layers.2.layer_norm2\n",
      "vision_model.encoder.layers.3\n",
      "vision_model.encoder.layers.3.self_attn\n",
      "vision_model.encoder.layers.3.self_attn.dropout\n",
      "vision_model.encoder.layers.3.self_attn.qkv\n",
      "vision_model.encoder.layers.3.self_attn.projection\n",
      "vision_model.encoder.layers.3.layer_norm1\n",
      "vision_model.encoder.layers.3.mlp\n",
      "vision_model.encoder.layers.3.mlp.activation_fn\n",
      "vision_model.encoder.layers.3.mlp.fc1\n",
      "vision_model.encoder.layers.3.mlp.fc2\n",
      "vision_model.encoder.layers.3.layer_norm2\n",
      "vision_model.encoder.layers.4\n",
      "vision_model.encoder.layers.4.self_attn\n",
      "vision_model.encoder.layers.4.self_attn.dropout\n",
      "vision_model.encoder.layers.4.self_attn.qkv\n",
      "vision_model.encoder.layers.4.self_attn.projection\n",
      "vision_model.encoder.layers.4.layer_norm1\n",
      "vision_model.encoder.layers.4.mlp\n",
      "vision_model.encoder.layers.4.mlp.activation_fn\n",
      "vision_model.encoder.layers.4.mlp.fc1\n",
      "vision_model.encoder.layers.4.mlp.fc2\n",
      "vision_model.encoder.layers.4.layer_norm2\n",
      "vision_model.encoder.layers.5\n",
      "vision_model.encoder.layers.5.self_attn\n",
      "vision_model.encoder.layers.5.self_attn.dropout\n",
      "vision_model.encoder.layers.5.self_attn.qkv\n",
      "vision_model.encoder.layers.5.self_attn.projection\n",
      "vision_model.encoder.layers.5.layer_norm1\n",
      "vision_model.encoder.layers.5.mlp\n",
      "vision_model.encoder.layers.5.mlp.activation_fn\n",
      "vision_model.encoder.layers.5.mlp.fc1\n",
      "vision_model.encoder.layers.5.mlp.fc2\n",
      "vision_model.encoder.layers.5.layer_norm2\n",
      "vision_model.encoder.layers.6\n",
      "vision_model.encoder.layers.6.self_attn\n",
      "vision_model.encoder.layers.6.self_attn.dropout\n",
      "vision_model.encoder.layers.6.self_attn.qkv\n",
      "vision_model.encoder.layers.6.self_attn.projection\n",
      "vision_model.encoder.layers.6.layer_norm1\n",
      "vision_model.encoder.layers.6.mlp\n",
      "vision_model.encoder.layers.6.mlp.activation_fn\n",
      "vision_model.encoder.layers.6.mlp.fc1\n",
      "vision_model.encoder.layers.6.mlp.fc2\n",
      "vision_model.encoder.layers.6.layer_norm2\n",
      "vision_model.encoder.layers.7\n",
      "vision_model.encoder.layers.7.self_attn\n",
      "vision_model.encoder.layers.7.self_attn.dropout\n",
      "vision_model.encoder.layers.7.self_attn.qkv\n",
      "vision_model.encoder.layers.7.self_attn.projection\n",
      "vision_model.encoder.layers.7.layer_norm1\n",
      "vision_model.encoder.layers.7.mlp\n",
      "vision_model.encoder.layers.7.mlp.activation_fn\n",
      "vision_model.encoder.layers.7.mlp.fc1\n",
      "vision_model.encoder.layers.7.mlp.fc2\n",
      "vision_model.encoder.layers.7.layer_norm2\n",
      "vision_model.encoder.layers.8\n",
      "vision_model.encoder.layers.8.self_attn\n",
      "vision_model.encoder.layers.8.self_attn.dropout\n",
      "vision_model.encoder.layers.8.self_attn.qkv\n",
      "vision_model.encoder.layers.8.self_attn.projection\n",
      "vision_model.encoder.layers.8.layer_norm1\n",
      "vision_model.encoder.layers.8.mlp\n",
      "vision_model.encoder.layers.8.mlp.activation_fn\n",
      "vision_model.encoder.layers.8.mlp.fc1\n",
      "vision_model.encoder.layers.8.mlp.fc2\n",
      "vision_model.encoder.layers.8.layer_norm2\n",
      "vision_model.encoder.layers.9\n",
      "vision_model.encoder.layers.9.self_attn\n",
      "vision_model.encoder.layers.9.self_attn.dropout\n",
      "vision_model.encoder.layers.9.self_attn.qkv\n",
      "vision_model.encoder.layers.9.self_attn.projection\n",
      "vision_model.encoder.layers.9.layer_norm1\n",
      "vision_model.encoder.layers.9.mlp\n",
      "vision_model.encoder.layers.9.mlp.activation_fn\n",
      "vision_model.encoder.layers.9.mlp.fc1\n",
      "vision_model.encoder.layers.9.mlp.fc2\n",
      "vision_model.encoder.layers.9.layer_norm2\n",
      "vision_model.encoder.layers.10\n",
      "vision_model.encoder.layers.10.self_attn\n",
      "vision_model.encoder.layers.10.self_attn.dropout\n",
      "vision_model.encoder.layers.10.self_attn.qkv\n",
      "vision_model.encoder.layers.10.self_attn.projection\n",
      "vision_model.encoder.layers.10.layer_norm1\n",
      "vision_model.encoder.layers.10.mlp\n",
      "vision_model.encoder.layers.10.mlp.activation_fn\n",
      "vision_model.encoder.layers.10.mlp.fc1\n",
      "vision_model.encoder.layers.10.mlp.fc2\n",
      "vision_model.encoder.layers.10.layer_norm2\n",
      "vision_model.encoder.layers.11\n",
      "vision_model.encoder.layers.11.self_attn\n",
      "vision_model.encoder.layers.11.self_attn.dropout\n",
      "vision_model.encoder.layers.11.self_attn.qkv\n",
      "vision_model.encoder.layers.11.self_attn.projection\n",
      "vision_model.encoder.layers.11.layer_norm1\n",
      "vision_model.encoder.layers.11.mlp\n",
      "vision_model.encoder.layers.11.mlp.activation_fn\n",
      "vision_model.encoder.layers.11.mlp.fc1\n",
      "vision_model.encoder.layers.11.mlp.fc2\n",
      "vision_model.encoder.layers.11.layer_norm2\n",
      "vision_model.post_layernorm\n",
      "text_encoder\n",
      "text_encoder.embeddings\n",
      "text_encoder.embeddings.word_embeddings\n",
      "text_encoder.embeddings.position_embeddings\n",
      "text_encoder.embeddings.LayerNorm\n",
      "text_encoder.embeddings.dropout\n",
      "text_encoder.encoder\n",
      "text_encoder.encoder.layer\n",
      "text_encoder.encoder.layer.0\n",
      "text_encoder.encoder.layer.0.attention\n",
      "text_encoder.encoder.layer.0.attention.self\n",
      "text_encoder.encoder.layer.0.attention.self.query\n",
      "text_encoder.encoder.layer.0.attention.self.key\n",
      "text_encoder.encoder.layer.0.attention.self.value\n",
      "text_encoder.encoder.layer.0.attention.self.dropout\n",
      "text_encoder.encoder.layer.0.attention.output\n",
      "text_encoder.encoder.layer.0.attention.output.dense\n",
      "text_encoder.encoder.layer.0.attention.output.LayerNorm\n",
      "text_encoder.encoder.layer.0.attention.output.dropout\n",
      "text_encoder.encoder.layer.0.crossattention\n",
      "text_encoder.encoder.layer.0.crossattention.self\n",
      "text_encoder.encoder.layer.0.crossattention.self.query\n",
      "text_encoder.encoder.layer.0.crossattention.self.key\n",
      "text_encoder.encoder.layer.0.crossattention.self.value\n",
      "text_encoder.encoder.layer.0.crossattention.self.dropout\n",
      "text_encoder.encoder.layer.0.crossattention.output\n",
      "text_encoder.encoder.layer.0.crossattention.output.dense\n",
      "text_encoder.encoder.layer.0.crossattention.output.LayerNorm\n",
      "text_encoder.encoder.layer.0.crossattention.output.dropout\n",
      "text_encoder.encoder.layer.0.intermediate\n",
      "text_encoder.encoder.layer.0.intermediate.dense\n",
      "text_encoder.encoder.layer.0.intermediate.intermediate_act_fn\n",
      "text_encoder.encoder.layer.0.output\n",
      "text_encoder.encoder.layer.0.output.dense\n",
      "text_encoder.encoder.layer.0.output.LayerNorm\n",
      "text_encoder.encoder.layer.0.output.dropout\n",
      "text_encoder.encoder.layer.1\n",
      "text_encoder.encoder.layer.1.attention\n",
      "text_encoder.encoder.layer.1.attention.self\n",
      "text_encoder.encoder.layer.1.attention.self.query\n",
      "text_encoder.encoder.layer.1.attention.self.key\n",
      "text_encoder.encoder.layer.1.attention.self.value\n",
      "text_encoder.encoder.layer.1.attention.self.dropout\n",
      "text_encoder.encoder.layer.1.attention.output\n",
      "text_encoder.encoder.layer.1.attention.output.dense\n",
      "text_encoder.encoder.layer.1.attention.output.LayerNorm\n",
      "text_encoder.encoder.layer.1.attention.output.dropout\n",
      "text_encoder.encoder.layer.1.crossattention\n",
      "text_encoder.encoder.layer.1.crossattention.self\n",
      "text_encoder.encoder.layer.1.crossattention.self.query\n",
      "text_encoder.encoder.layer.1.crossattention.self.key\n",
      "text_encoder.encoder.layer.1.crossattention.self.value\n",
      "text_encoder.encoder.layer.1.crossattention.self.dropout\n",
      "text_encoder.encoder.layer.1.crossattention.output\n",
      "text_encoder.encoder.layer.1.crossattention.output.dense\n",
      "text_encoder.encoder.layer.1.crossattention.output.LayerNorm\n",
      "text_encoder.encoder.layer.1.crossattention.output.dropout\n",
      "text_encoder.encoder.layer.1.intermediate\n",
      "text_encoder.encoder.layer.1.intermediate.dense\n",
      "text_encoder.encoder.layer.1.intermediate.intermediate_act_fn\n",
      "text_encoder.encoder.layer.1.output\n",
      "text_encoder.encoder.layer.1.output.dense\n",
      "text_encoder.encoder.layer.1.output.LayerNorm\n",
      "text_encoder.encoder.layer.1.output.dropout\n",
      "text_encoder.encoder.layer.2\n",
      "text_encoder.encoder.layer.2.attention\n",
      "text_encoder.encoder.layer.2.attention.self\n",
      "text_encoder.encoder.layer.2.attention.self.query\n",
      "text_encoder.encoder.layer.2.attention.self.key\n",
      "text_encoder.encoder.layer.2.attention.self.value\n",
      "text_encoder.encoder.layer.2.attention.self.dropout\n",
      "text_encoder.encoder.layer.2.attention.output\n",
      "text_encoder.encoder.layer.2.attention.output.dense\n",
      "text_encoder.encoder.layer.2.attention.output.LayerNorm\n",
      "text_encoder.encoder.layer.2.attention.output.dropout\n",
      "text_encoder.encoder.layer.2.crossattention\n",
      "text_encoder.encoder.layer.2.crossattention.self\n",
      "text_encoder.encoder.layer.2.crossattention.self.query\n",
      "text_encoder.encoder.layer.2.crossattention.self.key\n",
      "text_encoder.encoder.layer.2.crossattention.self.value\n",
      "text_encoder.encoder.layer.2.crossattention.self.dropout\n",
      "text_encoder.encoder.layer.2.crossattention.output\n",
      "text_encoder.encoder.layer.2.crossattention.output.dense\n",
      "text_encoder.encoder.layer.2.crossattention.output.LayerNorm\n",
      "text_encoder.encoder.layer.2.crossattention.output.dropout\n",
      "text_encoder.encoder.layer.2.intermediate\n",
      "text_encoder.encoder.layer.2.intermediate.dense\n",
      "text_encoder.encoder.layer.2.intermediate.intermediate_act_fn\n",
      "text_encoder.encoder.layer.2.output\n",
      "text_encoder.encoder.layer.2.output.dense\n",
      "text_encoder.encoder.layer.2.output.LayerNorm\n",
      "text_encoder.encoder.layer.2.output.dropout\n",
      "text_encoder.encoder.layer.3\n",
      "text_encoder.encoder.layer.3.attention\n",
      "text_encoder.encoder.layer.3.attention.self\n",
      "text_encoder.encoder.layer.3.attention.self.query\n",
      "text_encoder.encoder.layer.3.attention.self.key\n",
      "text_encoder.encoder.layer.3.attention.self.value\n",
      "text_encoder.encoder.layer.3.attention.self.dropout\n",
      "text_encoder.encoder.layer.3.attention.output\n",
      "text_encoder.encoder.layer.3.attention.output.dense\n",
      "text_encoder.encoder.layer.3.attention.output.LayerNorm\n",
      "text_encoder.encoder.layer.3.attention.output.dropout\n",
      "text_encoder.encoder.layer.3.crossattention\n",
      "text_encoder.encoder.layer.3.crossattention.self\n",
      "text_encoder.encoder.layer.3.crossattention.self.query\n",
      "text_encoder.encoder.layer.3.crossattention.self.key\n",
      "text_encoder.encoder.layer.3.crossattention.self.value\n",
      "text_encoder.encoder.layer.3.crossattention.self.dropout\n",
      "text_encoder.encoder.layer.3.crossattention.output\n",
      "text_encoder.encoder.layer.3.crossattention.output.dense\n",
      "text_encoder.encoder.layer.3.crossattention.output.LayerNorm\n",
      "text_encoder.encoder.layer.3.crossattention.output.dropout\n",
      "text_encoder.encoder.layer.3.intermediate\n",
      "text_encoder.encoder.layer.3.intermediate.dense\n",
      "text_encoder.encoder.layer.3.intermediate.intermediate_act_fn\n",
      "text_encoder.encoder.layer.3.output\n",
      "text_encoder.encoder.layer.3.output.dense\n",
      "text_encoder.encoder.layer.3.output.LayerNorm\n",
      "text_encoder.encoder.layer.3.output.dropout\n",
      "text_encoder.encoder.layer.4\n",
      "text_encoder.encoder.layer.4.attention\n",
      "text_encoder.encoder.layer.4.attention.self\n",
      "text_encoder.encoder.layer.4.attention.self.query\n",
      "text_encoder.encoder.layer.4.attention.self.key\n",
      "text_encoder.encoder.layer.4.attention.self.value\n",
      "text_encoder.encoder.layer.4.attention.self.dropout\n",
      "text_encoder.encoder.layer.4.attention.output\n",
      "text_encoder.encoder.layer.4.attention.output.dense\n",
      "text_encoder.encoder.layer.4.attention.output.LayerNorm\n",
      "text_encoder.encoder.layer.4.attention.output.dropout\n",
      "text_encoder.encoder.layer.4.crossattention\n",
      "text_encoder.encoder.layer.4.crossattention.self\n",
      "text_encoder.encoder.layer.4.crossattention.self.query\n",
      "text_encoder.encoder.layer.4.crossattention.self.key\n",
      "text_encoder.encoder.layer.4.crossattention.self.value\n",
      "text_encoder.encoder.layer.4.crossattention.self.dropout\n",
      "text_encoder.encoder.layer.4.crossattention.output\n",
      "text_encoder.encoder.layer.4.crossattention.output.dense\n",
      "text_encoder.encoder.layer.4.crossattention.output.LayerNorm\n",
      "text_encoder.encoder.layer.4.crossattention.output.dropout\n",
      "text_encoder.encoder.layer.4.intermediate\n",
      "text_encoder.encoder.layer.4.intermediate.dense\n",
      "text_encoder.encoder.layer.4.intermediate.intermediate_act_fn\n",
      "text_encoder.encoder.layer.4.output\n",
      "text_encoder.encoder.layer.4.output.dense\n",
      "text_encoder.encoder.layer.4.output.LayerNorm\n",
      "text_encoder.encoder.layer.4.output.dropout\n",
      "text_encoder.encoder.layer.5\n",
      "text_encoder.encoder.layer.5.attention\n",
      "text_encoder.encoder.layer.5.attention.self\n",
      "text_encoder.encoder.layer.5.attention.self.query\n",
      "text_encoder.encoder.layer.5.attention.self.key\n",
      "text_encoder.encoder.layer.5.attention.self.value\n",
      "text_encoder.encoder.layer.5.attention.self.dropout\n",
      "text_encoder.encoder.layer.5.attention.output\n",
      "text_encoder.encoder.layer.5.attention.output.dense\n",
      "text_encoder.encoder.layer.5.attention.output.LayerNorm\n",
      "text_encoder.encoder.layer.5.attention.output.dropout\n",
      "text_encoder.encoder.layer.5.crossattention\n",
      "text_encoder.encoder.layer.5.crossattention.self\n",
      "text_encoder.encoder.layer.5.crossattention.self.query\n",
      "text_encoder.encoder.layer.5.crossattention.self.key\n",
      "text_encoder.encoder.layer.5.crossattention.self.value\n",
      "text_encoder.encoder.layer.5.crossattention.self.dropout\n",
      "text_encoder.encoder.layer.5.crossattention.output\n",
      "text_encoder.encoder.layer.5.crossattention.output.dense\n",
      "text_encoder.encoder.layer.5.crossattention.output.LayerNorm\n",
      "text_encoder.encoder.layer.5.crossattention.output.dropout\n",
      "text_encoder.encoder.layer.5.intermediate\n",
      "text_encoder.encoder.layer.5.intermediate.dense\n",
      "text_encoder.encoder.layer.5.intermediate.intermediate_act_fn\n",
      "text_encoder.encoder.layer.5.output\n",
      "text_encoder.encoder.layer.5.output.dense\n",
      "text_encoder.encoder.layer.5.output.LayerNorm\n",
      "text_encoder.encoder.layer.5.output.dropout\n",
      "text_encoder.encoder.layer.6\n",
      "text_encoder.encoder.layer.6.attention\n",
      "text_encoder.encoder.layer.6.attention.self\n",
      "text_encoder.encoder.layer.6.attention.self.query\n",
      "text_encoder.encoder.layer.6.attention.self.key\n",
      "text_encoder.encoder.layer.6.attention.self.value\n",
      "text_encoder.encoder.layer.6.attention.self.dropout\n",
      "text_encoder.encoder.layer.6.attention.output\n",
      "text_encoder.encoder.layer.6.attention.output.dense\n",
      "text_encoder.encoder.layer.6.attention.output.LayerNorm\n",
      "text_encoder.encoder.layer.6.attention.output.dropout\n",
      "text_encoder.encoder.layer.6.crossattention\n",
      "text_encoder.encoder.layer.6.crossattention.self\n",
      "text_encoder.encoder.layer.6.crossattention.self.query\n",
      "text_encoder.encoder.layer.6.crossattention.self.key\n",
      "text_encoder.encoder.layer.6.crossattention.self.value\n",
      "text_encoder.encoder.layer.6.crossattention.self.dropout\n",
      "text_encoder.encoder.layer.6.crossattention.output\n",
      "text_encoder.encoder.layer.6.crossattention.output.dense\n",
      "text_encoder.encoder.layer.6.crossattention.output.LayerNorm\n",
      "text_encoder.encoder.layer.6.crossattention.output.dropout\n",
      "text_encoder.encoder.layer.6.intermediate\n",
      "text_encoder.encoder.layer.6.intermediate.dense\n",
      "text_encoder.encoder.layer.6.intermediate.intermediate_act_fn\n",
      "text_encoder.encoder.layer.6.output\n",
      "text_encoder.encoder.layer.6.output.dense\n",
      "text_encoder.encoder.layer.6.output.LayerNorm\n",
      "text_encoder.encoder.layer.6.output.dropout\n",
      "text_encoder.encoder.layer.7\n",
      "text_encoder.encoder.layer.7.attention\n",
      "text_encoder.encoder.layer.7.attention.self\n",
      "text_encoder.encoder.layer.7.attention.self.query\n",
      "text_encoder.encoder.layer.7.attention.self.key\n",
      "text_encoder.encoder.layer.7.attention.self.value\n",
      "text_encoder.encoder.layer.7.attention.self.dropout\n",
      "text_encoder.encoder.layer.7.attention.output\n",
      "text_encoder.encoder.layer.7.attention.output.dense\n",
      "text_encoder.encoder.layer.7.attention.output.LayerNorm\n",
      "text_encoder.encoder.layer.7.attention.output.dropout\n",
      "text_encoder.encoder.layer.7.crossattention\n",
      "text_encoder.encoder.layer.7.crossattention.self\n",
      "text_encoder.encoder.layer.7.crossattention.self.query\n",
      "text_encoder.encoder.layer.7.crossattention.self.key\n",
      "text_encoder.encoder.layer.7.crossattention.self.value\n",
      "text_encoder.encoder.layer.7.crossattention.self.dropout\n",
      "text_encoder.encoder.layer.7.crossattention.output\n",
      "text_encoder.encoder.layer.7.crossattention.output.dense\n",
      "text_encoder.encoder.layer.7.crossattention.output.LayerNorm\n",
      "text_encoder.encoder.layer.7.crossattention.output.dropout\n",
      "text_encoder.encoder.layer.7.intermediate\n",
      "text_encoder.encoder.layer.7.intermediate.dense\n",
      "text_encoder.encoder.layer.7.intermediate.intermediate_act_fn\n",
      "text_encoder.encoder.layer.7.output\n",
      "text_encoder.encoder.layer.7.output.dense\n",
      "text_encoder.encoder.layer.7.output.LayerNorm\n",
      "text_encoder.encoder.layer.7.output.dropout\n",
      "text_encoder.encoder.layer.8\n",
      "text_encoder.encoder.layer.8.attention\n",
      "text_encoder.encoder.layer.8.attention.self\n",
      "text_encoder.encoder.layer.8.attention.self.query\n",
      "text_encoder.encoder.layer.8.attention.self.key\n",
      "text_encoder.encoder.layer.8.attention.self.value\n",
      "text_encoder.encoder.layer.8.attention.self.dropout\n",
      "text_encoder.encoder.layer.8.attention.output\n",
      "text_encoder.encoder.layer.8.attention.output.dense\n",
      "text_encoder.encoder.layer.8.attention.output.LayerNorm\n",
      "text_encoder.encoder.layer.8.attention.output.dropout\n",
      "text_encoder.encoder.layer.8.crossattention\n",
      "text_encoder.encoder.layer.8.crossattention.self\n",
      "text_encoder.encoder.layer.8.crossattention.self.query\n",
      "text_encoder.encoder.layer.8.crossattention.self.key\n",
      "text_encoder.encoder.layer.8.crossattention.self.value\n",
      "text_encoder.encoder.layer.8.crossattention.self.dropout\n",
      "text_encoder.encoder.layer.8.crossattention.output\n",
      "text_encoder.encoder.layer.8.crossattention.output.dense\n",
      "text_encoder.encoder.layer.8.crossattention.output.LayerNorm\n",
      "text_encoder.encoder.layer.8.crossattention.output.dropout\n",
      "text_encoder.encoder.layer.8.intermediate\n",
      "text_encoder.encoder.layer.8.intermediate.dense\n",
      "text_encoder.encoder.layer.8.intermediate.intermediate_act_fn\n",
      "text_encoder.encoder.layer.8.output\n",
      "text_encoder.encoder.layer.8.output.dense\n",
      "text_encoder.encoder.layer.8.output.LayerNorm\n",
      "text_encoder.encoder.layer.8.output.dropout\n",
      "text_encoder.encoder.layer.9\n",
      "text_encoder.encoder.layer.9.attention\n",
      "text_encoder.encoder.layer.9.attention.self\n",
      "text_encoder.encoder.layer.9.attention.self.query\n",
      "text_encoder.encoder.layer.9.attention.self.key\n",
      "text_encoder.encoder.layer.9.attention.self.value\n",
      "text_encoder.encoder.layer.9.attention.self.dropout\n",
      "text_encoder.encoder.layer.9.attention.output\n",
      "text_encoder.encoder.layer.9.attention.output.dense\n",
      "text_encoder.encoder.layer.9.attention.output.LayerNorm\n",
      "text_encoder.encoder.layer.9.attention.output.dropout\n",
      "text_encoder.encoder.layer.9.crossattention\n",
      "text_encoder.encoder.layer.9.crossattention.self\n",
      "text_encoder.encoder.layer.9.crossattention.self.query\n",
      "text_encoder.encoder.layer.9.crossattention.self.key\n",
      "text_encoder.encoder.layer.9.crossattention.self.value\n",
      "text_encoder.encoder.layer.9.crossattention.self.dropout\n",
      "text_encoder.encoder.layer.9.crossattention.output\n",
      "text_encoder.encoder.layer.9.crossattention.output.dense\n",
      "text_encoder.encoder.layer.9.crossattention.output.LayerNorm\n",
      "text_encoder.encoder.layer.9.crossattention.output.dropout\n",
      "text_encoder.encoder.layer.9.intermediate\n",
      "text_encoder.encoder.layer.9.intermediate.dense\n",
      "text_encoder.encoder.layer.9.intermediate.intermediate_act_fn\n",
      "text_encoder.encoder.layer.9.output\n",
      "text_encoder.encoder.layer.9.output.dense\n",
      "text_encoder.encoder.layer.9.output.LayerNorm\n",
      "text_encoder.encoder.layer.9.output.dropout\n",
      "text_encoder.encoder.layer.10\n",
      "text_encoder.encoder.layer.10.attention\n",
      "text_encoder.encoder.layer.10.attention.self\n",
      "text_encoder.encoder.layer.10.attention.self.query\n",
      "text_encoder.encoder.layer.10.attention.self.key\n",
      "text_encoder.encoder.layer.10.attention.self.value\n",
      "text_encoder.encoder.layer.10.attention.self.dropout\n",
      "text_encoder.encoder.layer.10.attention.output\n",
      "text_encoder.encoder.layer.10.attention.output.dense\n",
      "text_encoder.encoder.layer.10.attention.output.LayerNorm\n",
      "text_encoder.encoder.layer.10.attention.output.dropout\n",
      "text_encoder.encoder.layer.10.crossattention\n",
      "text_encoder.encoder.layer.10.crossattention.self\n",
      "text_encoder.encoder.layer.10.crossattention.self.query\n",
      "text_encoder.encoder.layer.10.crossattention.self.key\n",
      "text_encoder.encoder.layer.10.crossattention.self.value\n",
      "text_encoder.encoder.layer.10.crossattention.self.dropout\n",
      "text_encoder.encoder.layer.10.crossattention.output\n",
      "text_encoder.encoder.layer.10.crossattention.output.dense\n",
      "text_encoder.encoder.layer.10.crossattention.output.LayerNorm\n",
      "text_encoder.encoder.layer.10.crossattention.output.dropout\n",
      "text_encoder.encoder.layer.10.intermediate\n",
      "text_encoder.encoder.layer.10.intermediate.dense\n",
      "text_encoder.encoder.layer.10.intermediate.intermediate_act_fn\n",
      "text_encoder.encoder.layer.10.output\n",
      "text_encoder.encoder.layer.10.output.dense\n",
      "text_encoder.encoder.layer.10.output.LayerNorm\n",
      "text_encoder.encoder.layer.10.output.dropout\n",
      "text_encoder.encoder.layer.11\n",
      "text_encoder.encoder.layer.11.attention\n",
      "text_encoder.encoder.layer.11.attention.self\n",
      "text_encoder.encoder.layer.11.attention.self.query\n",
      "text_encoder.encoder.layer.11.attention.self.key\n",
      "text_encoder.encoder.layer.11.attention.self.value\n",
      "text_encoder.encoder.layer.11.attention.self.dropout\n",
      "text_encoder.encoder.layer.11.attention.output\n",
      "text_encoder.encoder.layer.11.attention.output.dense\n",
      "text_encoder.encoder.layer.11.attention.output.LayerNorm\n",
      "text_encoder.encoder.layer.11.attention.output.dropout\n",
      "text_encoder.encoder.layer.11.crossattention\n",
      "text_encoder.encoder.layer.11.crossattention.self\n",
      "text_encoder.encoder.layer.11.crossattention.self.query\n",
      "text_encoder.encoder.layer.11.crossattention.self.key\n",
      "text_encoder.encoder.layer.11.crossattention.self.value\n",
      "text_encoder.encoder.layer.11.crossattention.self.dropout\n",
      "text_encoder.encoder.layer.11.crossattention.output\n",
      "text_encoder.encoder.layer.11.crossattention.output.dense\n",
      "text_encoder.encoder.layer.11.crossattention.output.LayerNorm\n",
      "text_encoder.encoder.layer.11.crossattention.output.dropout\n",
      "text_encoder.encoder.layer.11.intermediate\n",
      "text_encoder.encoder.layer.11.intermediate.dense\n",
      "text_encoder.encoder.layer.11.intermediate.intermediate_act_fn\n",
      "text_encoder.encoder.layer.11.output\n",
      "text_encoder.encoder.layer.11.output.dense\n",
      "text_encoder.encoder.layer.11.output.LayerNorm\n",
      "text_encoder.encoder.layer.11.output.dropout\n",
      "text_decoder\n",
      "text_decoder.bert\n",
      "text_decoder.bert.embeddings\n",
      "text_decoder.bert.embeddings.word_embeddings\n",
      "text_decoder.bert.embeddings.position_embeddings\n",
      "text_decoder.bert.embeddings.LayerNorm\n",
      "text_decoder.bert.embeddings.dropout\n",
      "text_decoder.bert.encoder\n",
      "text_decoder.bert.encoder.layer\n",
      "text_decoder.bert.encoder.layer.0\n",
      "text_decoder.bert.encoder.layer.0.attention\n",
      "text_decoder.bert.encoder.layer.0.attention.self\n",
      "text_decoder.bert.encoder.layer.0.attention.self.query\n",
      "text_decoder.bert.encoder.layer.0.attention.self.key\n",
      "text_decoder.bert.encoder.layer.0.attention.self.value\n",
      "text_decoder.bert.encoder.layer.0.attention.self.dropout\n",
      "text_decoder.bert.encoder.layer.0.attention.output\n",
      "text_decoder.bert.encoder.layer.0.attention.output.dense\n",
      "text_decoder.bert.encoder.layer.0.attention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.0.attention.output.dropout\n",
      "text_decoder.bert.encoder.layer.0.crossattention\n",
      "text_decoder.bert.encoder.layer.0.crossattention.self\n",
      "text_decoder.bert.encoder.layer.0.crossattention.self.query\n",
      "text_decoder.bert.encoder.layer.0.crossattention.self.key\n",
      "text_decoder.bert.encoder.layer.0.crossattention.self.value\n",
      "text_decoder.bert.encoder.layer.0.crossattention.self.dropout\n",
      "text_decoder.bert.encoder.layer.0.crossattention.output\n",
      "text_decoder.bert.encoder.layer.0.crossattention.output.dense\n",
      "text_decoder.bert.encoder.layer.0.crossattention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.0.crossattention.output.dropout\n",
      "text_decoder.bert.encoder.layer.0.intermediate\n",
      "text_decoder.bert.encoder.layer.0.intermediate.dense\n",
      "text_decoder.bert.encoder.layer.0.intermediate.intermediate_act_fn\n",
      "text_decoder.bert.encoder.layer.0.output\n",
      "text_decoder.bert.encoder.layer.0.output.dense\n",
      "text_decoder.bert.encoder.layer.0.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.0.output.dropout\n",
      "text_decoder.bert.encoder.layer.1\n",
      "text_decoder.bert.encoder.layer.1.attention\n",
      "text_decoder.bert.encoder.layer.1.attention.self\n",
      "text_decoder.bert.encoder.layer.1.attention.self.query\n",
      "text_decoder.bert.encoder.layer.1.attention.self.key\n",
      "text_decoder.bert.encoder.layer.1.attention.self.value\n",
      "text_decoder.bert.encoder.layer.1.attention.self.dropout\n",
      "text_decoder.bert.encoder.layer.1.attention.output\n",
      "text_decoder.bert.encoder.layer.1.attention.output.dense\n",
      "text_decoder.bert.encoder.layer.1.attention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.1.attention.output.dropout\n",
      "text_decoder.bert.encoder.layer.1.crossattention\n",
      "text_decoder.bert.encoder.layer.1.crossattention.self\n",
      "text_decoder.bert.encoder.layer.1.crossattention.self.query\n",
      "text_decoder.bert.encoder.layer.1.crossattention.self.key\n",
      "text_decoder.bert.encoder.layer.1.crossattention.self.value\n",
      "text_decoder.bert.encoder.layer.1.crossattention.self.dropout\n",
      "text_decoder.bert.encoder.layer.1.crossattention.output\n",
      "text_decoder.bert.encoder.layer.1.crossattention.output.dense\n",
      "text_decoder.bert.encoder.layer.1.crossattention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.1.crossattention.output.dropout\n",
      "text_decoder.bert.encoder.layer.1.intermediate\n",
      "text_decoder.bert.encoder.layer.1.intermediate.dense\n",
      "text_decoder.bert.encoder.layer.1.intermediate.intermediate_act_fn\n",
      "text_decoder.bert.encoder.layer.1.output\n",
      "text_decoder.bert.encoder.layer.1.output.dense\n",
      "text_decoder.bert.encoder.layer.1.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.1.output.dropout\n",
      "text_decoder.bert.encoder.layer.2\n",
      "text_decoder.bert.encoder.layer.2.attention\n",
      "text_decoder.bert.encoder.layer.2.attention.self\n",
      "text_decoder.bert.encoder.layer.2.attention.self.query\n",
      "text_decoder.bert.encoder.layer.2.attention.self.key\n",
      "text_decoder.bert.encoder.layer.2.attention.self.value\n",
      "text_decoder.bert.encoder.layer.2.attention.self.dropout\n",
      "text_decoder.bert.encoder.layer.2.attention.output\n",
      "text_decoder.bert.encoder.layer.2.attention.output.dense\n",
      "text_decoder.bert.encoder.layer.2.attention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.2.attention.output.dropout\n",
      "text_decoder.bert.encoder.layer.2.crossattention\n",
      "text_decoder.bert.encoder.layer.2.crossattention.self\n",
      "text_decoder.bert.encoder.layer.2.crossattention.self.query\n",
      "text_decoder.bert.encoder.layer.2.crossattention.self.key\n",
      "text_decoder.bert.encoder.layer.2.crossattention.self.value\n",
      "text_decoder.bert.encoder.layer.2.crossattention.self.dropout\n",
      "text_decoder.bert.encoder.layer.2.crossattention.output\n",
      "text_decoder.bert.encoder.layer.2.crossattention.output.dense\n",
      "text_decoder.bert.encoder.layer.2.crossattention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.2.crossattention.output.dropout\n",
      "text_decoder.bert.encoder.layer.2.intermediate\n",
      "text_decoder.bert.encoder.layer.2.intermediate.dense\n",
      "text_decoder.bert.encoder.layer.2.intermediate.intermediate_act_fn\n",
      "text_decoder.bert.encoder.layer.2.output\n",
      "text_decoder.bert.encoder.layer.2.output.dense\n",
      "text_decoder.bert.encoder.layer.2.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.2.output.dropout\n",
      "text_decoder.bert.encoder.layer.3\n",
      "text_decoder.bert.encoder.layer.3.attention\n",
      "text_decoder.bert.encoder.layer.3.attention.self\n",
      "text_decoder.bert.encoder.layer.3.attention.self.query\n",
      "text_decoder.bert.encoder.layer.3.attention.self.key\n",
      "text_decoder.bert.encoder.layer.3.attention.self.value\n",
      "text_decoder.bert.encoder.layer.3.attention.self.dropout\n",
      "text_decoder.bert.encoder.layer.3.attention.output\n",
      "text_decoder.bert.encoder.layer.3.attention.output.dense\n",
      "text_decoder.bert.encoder.layer.3.attention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.3.attention.output.dropout\n",
      "text_decoder.bert.encoder.layer.3.crossattention\n",
      "text_decoder.bert.encoder.layer.3.crossattention.self\n",
      "text_decoder.bert.encoder.layer.3.crossattention.self.query\n",
      "text_decoder.bert.encoder.layer.3.crossattention.self.key\n",
      "text_decoder.bert.encoder.layer.3.crossattention.self.value\n",
      "text_decoder.bert.encoder.layer.3.crossattention.self.dropout\n",
      "text_decoder.bert.encoder.layer.3.crossattention.output\n",
      "text_decoder.bert.encoder.layer.3.crossattention.output.dense\n",
      "text_decoder.bert.encoder.layer.3.crossattention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.3.crossattention.output.dropout\n",
      "text_decoder.bert.encoder.layer.3.intermediate\n",
      "text_decoder.bert.encoder.layer.3.intermediate.dense\n",
      "text_decoder.bert.encoder.layer.3.intermediate.intermediate_act_fn\n",
      "text_decoder.bert.encoder.layer.3.output\n",
      "text_decoder.bert.encoder.layer.3.output.dense\n",
      "text_decoder.bert.encoder.layer.3.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.3.output.dropout\n",
      "text_decoder.bert.encoder.layer.4\n",
      "text_decoder.bert.encoder.layer.4.attention\n",
      "text_decoder.bert.encoder.layer.4.attention.self\n",
      "text_decoder.bert.encoder.layer.4.attention.self.query\n",
      "text_decoder.bert.encoder.layer.4.attention.self.key\n",
      "text_decoder.bert.encoder.layer.4.attention.self.value\n",
      "text_decoder.bert.encoder.layer.4.attention.self.dropout\n",
      "text_decoder.bert.encoder.layer.4.attention.output\n",
      "text_decoder.bert.encoder.layer.4.attention.output.dense\n",
      "text_decoder.bert.encoder.layer.4.attention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.4.attention.output.dropout\n",
      "text_decoder.bert.encoder.layer.4.crossattention\n",
      "text_decoder.bert.encoder.layer.4.crossattention.self\n",
      "text_decoder.bert.encoder.layer.4.crossattention.self.query\n",
      "text_decoder.bert.encoder.layer.4.crossattention.self.key\n",
      "text_decoder.bert.encoder.layer.4.crossattention.self.value\n",
      "text_decoder.bert.encoder.layer.4.crossattention.self.dropout\n",
      "text_decoder.bert.encoder.layer.4.crossattention.output\n",
      "text_decoder.bert.encoder.layer.4.crossattention.output.dense\n",
      "text_decoder.bert.encoder.layer.4.crossattention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.4.crossattention.output.dropout\n",
      "text_decoder.bert.encoder.layer.4.intermediate\n",
      "text_decoder.bert.encoder.layer.4.intermediate.dense\n",
      "text_decoder.bert.encoder.layer.4.intermediate.intermediate_act_fn\n",
      "text_decoder.bert.encoder.layer.4.output\n",
      "text_decoder.bert.encoder.layer.4.output.dense\n",
      "text_decoder.bert.encoder.layer.4.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.4.output.dropout\n",
      "text_decoder.bert.encoder.layer.5\n",
      "text_decoder.bert.encoder.layer.5.attention\n",
      "text_decoder.bert.encoder.layer.5.attention.self\n",
      "text_decoder.bert.encoder.layer.5.attention.self.query\n",
      "text_decoder.bert.encoder.layer.5.attention.self.key\n",
      "text_decoder.bert.encoder.layer.5.attention.self.value\n",
      "text_decoder.bert.encoder.layer.5.attention.self.dropout\n",
      "text_decoder.bert.encoder.layer.5.attention.output\n",
      "text_decoder.bert.encoder.layer.5.attention.output.dense\n",
      "text_decoder.bert.encoder.layer.5.attention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.5.attention.output.dropout\n",
      "text_decoder.bert.encoder.layer.5.crossattention\n",
      "text_decoder.bert.encoder.layer.5.crossattention.self\n",
      "text_decoder.bert.encoder.layer.5.crossattention.self.query\n",
      "text_decoder.bert.encoder.layer.5.crossattention.self.key\n",
      "text_decoder.bert.encoder.layer.5.crossattention.self.value\n",
      "text_decoder.bert.encoder.layer.5.crossattention.self.dropout\n",
      "text_decoder.bert.encoder.layer.5.crossattention.output\n",
      "text_decoder.bert.encoder.layer.5.crossattention.output.dense\n",
      "text_decoder.bert.encoder.layer.5.crossattention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.5.crossattention.output.dropout\n",
      "text_decoder.bert.encoder.layer.5.intermediate\n",
      "text_decoder.bert.encoder.layer.5.intermediate.dense\n",
      "text_decoder.bert.encoder.layer.5.intermediate.intermediate_act_fn\n",
      "text_decoder.bert.encoder.layer.5.output\n",
      "text_decoder.bert.encoder.layer.5.output.dense\n",
      "text_decoder.bert.encoder.layer.5.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.5.output.dropout\n",
      "text_decoder.bert.encoder.layer.6\n",
      "text_decoder.bert.encoder.layer.6.attention\n",
      "text_decoder.bert.encoder.layer.6.attention.self\n",
      "text_decoder.bert.encoder.layer.6.attention.self.query\n",
      "text_decoder.bert.encoder.layer.6.attention.self.key\n",
      "text_decoder.bert.encoder.layer.6.attention.self.value\n",
      "text_decoder.bert.encoder.layer.6.attention.self.dropout\n",
      "text_decoder.bert.encoder.layer.6.attention.output\n",
      "text_decoder.bert.encoder.layer.6.attention.output.dense\n",
      "text_decoder.bert.encoder.layer.6.attention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.6.attention.output.dropout\n",
      "text_decoder.bert.encoder.layer.6.crossattention\n",
      "text_decoder.bert.encoder.layer.6.crossattention.self\n",
      "text_decoder.bert.encoder.layer.6.crossattention.self.query\n",
      "text_decoder.bert.encoder.layer.6.crossattention.self.key\n",
      "text_decoder.bert.encoder.layer.6.crossattention.self.value\n",
      "text_decoder.bert.encoder.layer.6.crossattention.self.dropout\n",
      "text_decoder.bert.encoder.layer.6.crossattention.output\n",
      "text_decoder.bert.encoder.layer.6.crossattention.output.dense\n",
      "text_decoder.bert.encoder.layer.6.crossattention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.6.crossattention.output.dropout\n",
      "text_decoder.bert.encoder.layer.6.intermediate\n",
      "text_decoder.bert.encoder.layer.6.intermediate.dense\n",
      "text_decoder.bert.encoder.layer.6.intermediate.intermediate_act_fn\n",
      "text_decoder.bert.encoder.layer.6.output\n",
      "text_decoder.bert.encoder.layer.6.output.dense\n",
      "text_decoder.bert.encoder.layer.6.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.6.output.dropout\n",
      "text_decoder.bert.encoder.layer.7\n",
      "text_decoder.bert.encoder.layer.7.attention\n",
      "text_decoder.bert.encoder.layer.7.attention.self\n",
      "text_decoder.bert.encoder.layer.7.attention.self.query\n",
      "text_decoder.bert.encoder.layer.7.attention.self.key\n",
      "text_decoder.bert.encoder.layer.7.attention.self.value\n",
      "text_decoder.bert.encoder.layer.7.attention.self.dropout\n",
      "text_decoder.bert.encoder.layer.7.attention.output\n",
      "text_decoder.bert.encoder.layer.7.attention.output.dense\n",
      "text_decoder.bert.encoder.layer.7.attention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.7.attention.output.dropout\n",
      "text_decoder.bert.encoder.layer.7.crossattention\n",
      "text_decoder.bert.encoder.layer.7.crossattention.self\n",
      "text_decoder.bert.encoder.layer.7.crossattention.self.query\n",
      "text_decoder.bert.encoder.layer.7.crossattention.self.key\n",
      "text_decoder.bert.encoder.layer.7.crossattention.self.value\n",
      "text_decoder.bert.encoder.layer.7.crossattention.self.dropout\n",
      "text_decoder.bert.encoder.layer.7.crossattention.output\n",
      "text_decoder.bert.encoder.layer.7.crossattention.output.dense\n",
      "text_decoder.bert.encoder.layer.7.crossattention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.7.crossattention.output.dropout\n",
      "text_decoder.bert.encoder.layer.7.intermediate\n",
      "text_decoder.bert.encoder.layer.7.intermediate.dense\n",
      "text_decoder.bert.encoder.layer.7.intermediate.intermediate_act_fn\n",
      "text_decoder.bert.encoder.layer.7.output\n",
      "text_decoder.bert.encoder.layer.7.output.dense\n",
      "text_decoder.bert.encoder.layer.7.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.7.output.dropout\n",
      "text_decoder.bert.encoder.layer.8\n",
      "text_decoder.bert.encoder.layer.8.attention\n",
      "text_decoder.bert.encoder.layer.8.attention.self\n",
      "text_decoder.bert.encoder.layer.8.attention.self.query\n",
      "text_decoder.bert.encoder.layer.8.attention.self.key\n",
      "text_decoder.bert.encoder.layer.8.attention.self.value\n",
      "text_decoder.bert.encoder.layer.8.attention.self.dropout\n",
      "text_decoder.bert.encoder.layer.8.attention.output\n",
      "text_decoder.bert.encoder.layer.8.attention.output.dense\n",
      "text_decoder.bert.encoder.layer.8.attention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.8.attention.output.dropout\n",
      "text_decoder.bert.encoder.layer.8.crossattention\n",
      "text_decoder.bert.encoder.layer.8.crossattention.self\n",
      "text_decoder.bert.encoder.layer.8.crossattention.self.query\n",
      "text_decoder.bert.encoder.layer.8.crossattention.self.key\n",
      "text_decoder.bert.encoder.layer.8.crossattention.self.value\n",
      "text_decoder.bert.encoder.layer.8.crossattention.self.dropout\n",
      "text_decoder.bert.encoder.layer.8.crossattention.output\n",
      "text_decoder.bert.encoder.layer.8.crossattention.output.dense\n",
      "text_decoder.bert.encoder.layer.8.crossattention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.8.crossattention.output.dropout\n",
      "text_decoder.bert.encoder.layer.8.intermediate\n",
      "text_decoder.bert.encoder.layer.8.intermediate.dense\n",
      "text_decoder.bert.encoder.layer.8.intermediate.intermediate_act_fn\n",
      "text_decoder.bert.encoder.layer.8.output\n",
      "text_decoder.bert.encoder.layer.8.output.dense\n",
      "text_decoder.bert.encoder.layer.8.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.8.output.dropout\n",
      "text_decoder.bert.encoder.layer.9\n",
      "text_decoder.bert.encoder.layer.9.attention\n",
      "text_decoder.bert.encoder.layer.9.attention.self\n",
      "text_decoder.bert.encoder.layer.9.attention.self.query\n",
      "text_decoder.bert.encoder.layer.9.attention.self.key\n",
      "text_decoder.bert.encoder.layer.9.attention.self.value\n",
      "text_decoder.bert.encoder.layer.9.attention.self.dropout\n",
      "text_decoder.bert.encoder.layer.9.attention.output\n",
      "text_decoder.bert.encoder.layer.9.attention.output.dense\n",
      "text_decoder.bert.encoder.layer.9.attention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.9.attention.output.dropout\n",
      "text_decoder.bert.encoder.layer.9.crossattention\n",
      "text_decoder.bert.encoder.layer.9.crossattention.self\n",
      "text_decoder.bert.encoder.layer.9.crossattention.self.query\n",
      "text_decoder.bert.encoder.layer.9.crossattention.self.key\n",
      "text_decoder.bert.encoder.layer.9.crossattention.self.value\n",
      "text_decoder.bert.encoder.layer.9.crossattention.self.dropout\n",
      "text_decoder.bert.encoder.layer.9.crossattention.output\n",
      "text_decoder.bert.encoder.layer.9.crossattention.output.dense\n",
      "text_decoder.bert.encoder.layer.9.crossattention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.9.crossattention.output.dropout\n",
      "text_decoder.bert.encoder.layer.9.intermediate\n",
      "text_decoder.bert.encoder.layer.9.intermediate.dense\n",
      "text_decoder.bert.encoder.layer.9.intermediate.intermediate_act_fn\n",
      "text_decoder.bert.encoder.layer.9.output\n",
      "text_decoder.bert.encoder.layer.9.output.dense\n",
      "text_decoder.bert.encoder.layer.9.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.9.output.dropout\n",
      "text_decoder.bert.encoder.layer.10\n",
      "text_decoder.bert.encoder.layer.10.attention\n",
      "text_decoder.bert.encoder.layer.10.attention.self\n",
      "text_decoder.bert.encoder.layer.10.attention.self.query\n",
      "text_decoder.bert.encoder.layer.10.attention.self.key\n",
      "text_decoder.bert.encoder.layer.10.attention.self.value\n",
      "text_decoder.bert.encoder.layer.10.attention.self.dropout\n",
      "text_decoder.bert.encoder.layer.10.attention.output\n",
      "text_decoder.bert.encoder.layer.10.attention.output.dense\n",
      "text_decoder.bert.encoder.layer.10.attention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.10.attention.output.dropout\n",
      "text_decoder.bert.encoder.layer.10.crossattention\n",
      "text_decoder.bert.encoder.layer.10.crossattention.self\n",
      "text_decoder.bert.encoder.layer.10.crossattention.self.query\n",
      "text_decoder.bert.encoder.layer.10.crossattention.self.key\n",
      "text_decoder.bert.encoder.layer.10.crossattention.self.value\n",
      "text_decoder.bert.encoder.layer.10.crossattention.self.dropout\n",
      "text_decoder.bert.encoder.layer.10.crossattention.output\n",
      "text_decoder.bert.encoder.layer.10.crossattention.output.dense\n",
      "text_decoder.bert.encoder.layer.10.crossattention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.10.crossattention.output.dropout\n",
      "text_decoder.bert.encoder.layer.10.intermediate\n",
      "text_decoder.bert.encoder.layer.10.intermediate.dense\n",
      "text_decoder.bert.encoder.layer.10.intermediate.intermediate_act_fn\n",
      "text_decoder.bert.encoder.layer.10.output\n",
      "text_decoder.bert.encoder.layer.10.output.dense\n",
      "text_decoder.bert.encoder.layer.10.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.10.output.dropout\n",
      "text_decoder.bert.encoder.layer.11\n",
      "text_decoder.bert.encoder.layer.11.attention\n",
      "text_decoder.bert.encoder.layer.11.attention.self\n",
      "text_decoder.bert.encoder.layer.11.attention.self.query\n",
      "text_decoder.bert.encoder.layer.11.attention.self.key\n",
      "text_decoder.bert.encoder.layer.11.attention.self.value\n",
      "text_decoder.bert.encoder.layer.11.attention.self.dropout\n",
      "text_decoder.bert.encoder.layer.11.attention.output\n",
      "text_decoder.bert.encoder.layer.11.attention.output.dense\n",
      "text_decoder.bert.encoder.layer.11.attention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.11.attention.output.dropout\n",
      "text_decoder.bert.encoder.layer.11.crossattention\n",
      "text_decoder.bert.encoder.layer.11.crossattention.self\n",
      "text_decoder.bert.encoder.layer.11.crossattention.self.query\n",
      "text_decoder.bert.encoder.layer.11.crossattention.self.key\n",
      "text_decoder.bert.encoder.layer.11.crossattention.self.value\n",
      "text_decoder.bert.encoder.layer.11.crossattention.self.dropout\n",
      "text_decoder.bert.encoder.layer.11.crossattention.output\n",
      "text_decoder.bert.encoder.layer.11.crossattention.output.dense\n",
      "text_decoder.bert.encoder.layer.11.crossattention.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.11.crossattention.output.dropout\n",
      "text_decoder.bert.encoder.layer.11.intermediate\n",
      "text_decoder.bert.encoder.layer.11.intermediate.dense\n",
      "text_decoder.bert.encoder.layer.11.intermediate.intermediate_act_fn\n",
      "text_decoder.bert.encoder.layer.11.output\n",
      "text_decoder.bert.encoder.layer.11.output.dense\n",
      "text_decoder.bert.encoder.layer.11.output.LayerNorm\n",
      "text_decoder.bert.encoder.layer.11.output.dropout\n",
      "text_decoder.cls\n",
      "text_decoder.cls.predictions\n",
      "text_decoder.cls.predictions.transform\n",
      "text_decoder.cls.predictions.transform.dense\n",
      "text_decoder.cls.predictions.transform.transform_act_fn\n",
      "text_decoder.cls.predictions.transform.LayerNorm\n",
      "text_decoder.cls.predictions.decoder\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_target_modules = [\n",
    "    f\"text_encoder.encoder.layer.{i}.attention.self.query\" for i in range(11, 12)\n",
    "] + [\n",
    "    # f\"text_encoder.encoder.layer.{i}.attention.self.key\" for i in range(11, 13)\n",
    "] + [\n",
    "    # f\"text_encoder.encoder.layer.{i}.attention.self.value\" for i in range(11, 13)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,144 || all params: 384,678,716 || trainable%: 0.0016\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=4, # rank of the low-rank matrices that LoRA uses to modify certain parts of the model\n",
    "    lora_alpha=32, # scale LoRA weights, effectively controlling how strongly the LoRA layers impact the model’s predictions\n",
    "    lora_dropout=0.05, # \n",
    "    bias=\"none\", # whether or not to add biases to the LoRA-modified layers\n",
    "    target_modules=text_encoder_target_modules       # specifies the exact layers in the model where LoRA should be applied, typically in attention mechanisms\n",
    "                                        # [\"q_proj\", \"k_proj\"] are standard as they affect the query and key projections in attention layers\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, epoch, min_eval_loss, early_stopping_hook, file_path=\"checkpoint.pth\"):\n",
    "    # Save the model, optimizer, scheduler states, and epoch information\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'min_eval_loss': min_eval_loss,\n",
    "        'early_stopping_hook': early_stopping_hook\n",
    "    }, file_path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch} to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, scheduler, file_path=\"checkpoint.pth\"):\n",
    "    # Load the model, optimizer, scheduler states, and epoch information if available\n",
    "    if os.path.isfile(file_path):\n",
    "        checkpoint = torch.load(file_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        min_eval_loss = checkpoint['min_eval_loss']\n",
    "        early_stopping_hook = checkpoint['early_stopping_hook']\n",
    "        print(f\"Resumed training from epoch {epoch}\")\n",
    "        return epoch, min_eval_loss, early_stopping_hook\n",
    "    return 0, float(\"inf\"), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "def train(model, train_dataloader, valid_dataloader, num_epochs, optimizer, patience, scheduler, checkpoint_path=\"checkpoint.pth\"):\n",
    "    # Load checkpoint if exists\n",
    "    start_epoch, min_eval_loss, early_stopping_hook = load_checkpoint(model, optimizer, scheduler, checkpoint_path)\n",
    "    scaler = torch.amp.GradScaler(\"cuda\")  # Updated as per latest method\n",
    "    tracking_information = []\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for idx, batch in zip(tqdm(range(len(train_dataloader)), desc='Training batch: ...'), train_dataloader):\n",
    "            input_ids = batch.pop('input_ids').to(device)\n",
    "            pixel_values = batch.pop('pixel_values').to(device)\n",
    "            attention_masked = batch.pop('attention_mask').to(device)\n",
    "            labels = batch.pop('labels').to(device)\n",
    "            \n",
    "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(input_ids=input_ids,\n",
    "                            pixel_values=pixel_values,\n",
    "                            labels=labels)\n",
    "                \n",
    "            loss = outputs.loss\n",
    "            epoch_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        \n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        for idx, batch in zip(tqdm(range(len(valid_dataloader)), desc='Validating batch: ...'), valid_dataloader):\n",
    "            input_ids = batch.pop('input_ids').to(device)\n",
    "            pixel_values = batch.pop('pixel_values').to(device)\n",
    "            attention_masked = batch.pop('attention_mask').to(device)\n",
    "            labels = batch.pop('labels').to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(input_ids=input_ids,\n",
    "                            pixel_values=pixel_values,\n",
    "                            attention_mask=attention_masked,\n",
    "                            labels=labels)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "        tracking_information.append((epoch_loss/len(train_dataloader), eval_loss/len(valid_dataloader), optimizer.param_groups[0][\"lr\"]))\n",
    "        print(\"Epoch: {} - Training loss: {} - Eval Loss: {} - LR: {}\".format(epoch+1, epoch_loss/len(train_dataloader), eval_loss/len(valid_dataloader), optimizer.param_groups[0][\"lr\"]))\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save model if it has the lowest evaluation loss\n",
    "        if eval_loss < min_eval_loss:\n",
    "            saved_path = os.path.join('BLIP_tuned_model')\n",
    "            model.save_pretrained(saved_path, from_pt=True) \n",
    "            print(f'Saved model to {os.path.abspath(saved_path)}')\n",
    "            min_eval_loss = eval_loss\n",
    "            early_stopping_hook = 0\n",
    "        else:\n",
    "            early_stopping_hook += 1\n",
    "            if early_stopping_hook > patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Save checkpoint at the end of each epoch\n",
    "        save_checkpoint(model, optimizer, scheduler, epoch + 1, min_eval_loss, early_stopping_hook, checkpoint_path)\n",
    "    \n",
    "    pickle.dump(tracking_information, open(os.path.join('BLIP_tuned_model', 'tracking_info.pkl'), \"wb\"))\n",
    "    print(\"The finetuning process has done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "def train(model, train_dataloader, valid_dataloader, num_epochs, optimizer, patience, scheduler):\n",
    "    min_eval_loss = float(\"inf\")\n",
    "    early_stopping_hook = 0\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    tracking_information = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for idx, batch in zip(tqdm(range(len(train_dataloader)), desc='Training batch: ...'), train_dataloader):\n",
    "            input_ids = batch.pop('input_ids').to(device)\n",
    "            pixel_values = batch.pop('pixel_values').to(device)\n",
    "            attention_masked = batch.pop('attention_mask').to(device)\n",
    "            labels = batch.pop('labels').to(device)\n",
    "            \n",
    "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(input_ids=input_ids,\n",
    "                            pixel_values=pixel_values,\n",
    "                            # attention_mask=attention_masked,\n",
    "                            labels=labels)\n",
    "                \n",
    "            loss = outputs.loss\n",
    "            epoch_loss += loss.item()\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        \n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        for idx, batch in zip(tqdm(range(len(valid_dataloader)), desc='Validating batch: ...'), valid_dataloader):\n",
    "            input_ids = batch.pop('input_ids').to(device)\n",
    "            pixel_values = batch.pop('pixel_values').to(device)\n",
    "            attention_masked = batch.pop('attention_mask').to(device)\n",
    "            labels = batch.pop('labels').to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(input_ids=input_ids,\n",
    "                            pixel_values=pixel_values,\n",
    "                            attention_mask=attention_masked,\n",
    "                            labels=labels)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "        tracking_information.append((epoch_loss/len(train_dataloader), eval_loss/len(valid_dataloader), optimizer.param_groups[0][\"lr\"]))\n",
    "        print(\"Epoch: {} - Training loss: {} - Eval Loss: {} - LR: {}\".format(epoch+1, epoch_loss/len(train_dataloader), eval_loss/len(valid_dataloader), optimizer.param_groups[0][\"lr\"]))\n",
    "        scheduler.step()\n",
    "        if eval_loss < min_eval_loss:\n",
    "            saved_path = os.path.join('BLIP_tuned_model')\n",
    "            model.save_pretrained(saved_path, from_pt=True) \n",
    "            print(f'Saved model to {os.path.abspath(saved_path)}')\n",
    "            min_eval_loss = eval_loss\n",
    "            early_stopping_hook = 0\n",
    "        else:\n",
    "            early_stopping_hook += 1\n",
    "            if early_stopping_hook > patience:\n",
    "                break\n",
    "    pickle.dump(tracking_information, open(os.path.join('BLIP_tuned_model'), \"wb\"))\n",
    "    print(\"The finetuning process has done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: ...:   0%|                                                                   | 0/12504 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Training batch: ...:   7%|███▋                                                   | 835/12504 [10:20<2:24:26,  1.35it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_LAUNCH_BLOCKING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m---> 13\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPATIENCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_path\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, valid_dataloader, num_epochs, optimizer, patience, scheduler, checkpoint_path)\u001b[0m\n\u001b[0;32m     20\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m     21\u001b[0m                 pixel_values\u001b[38;5;241m=\u001b[39mpixel_values,\n\u001b[0;32m     22\u001b[0m                 labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m---> 25\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     28\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, last_epoch=-1, verbose=False)\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "PATIENCE = 10\n",
    "\n",
    "checkpoint_path = os.path.join('BLIP_tuned_model')\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "torch.cuda.empty_cache()\n",
    "train(model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    valid_dataloader=valid_dataloader,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    optimizer=optimizer,\n",
    "    patience=PATIENCE,\n",
    "    scheduler=scheduler,\n",
    "    checkpoint_path=checkpoint_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BLIP_env",
   "language": "python",
   "name": "blip_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
